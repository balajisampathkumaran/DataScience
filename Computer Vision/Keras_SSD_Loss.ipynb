{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "The Keras-compatible loss function for the SSD model. Currently supports TensorFlow only.\n",
    "\n",
    "Copyright (C) 2017 Pierluigi Ferrari\n",
    "\n",
    "This program is free software: you can redistribute it and/or modify\n",
    "it under the terms of the GNU General Public License as published by\n",
    "the Free Software Foundation, either version 3 of the License, or\n",
    "(at your option) any later version.\n",
    "\n",
    "This program is distributed in the hope that it will be useful,\n",
    "but WITHOUT ANY WARRANTY; without even the implied warranty of\n",
    "MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n",
    "GNU General Public License for more details.\n",
    "\n",
    "You should have received a copy of the GNU General Public License\n",
    "along with this program.  If not, see <http://www.gnu.org/licenses/>.\n",
    "'''\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "class SSDLoss:\n",
    "    '''\n",
    "    The SSD loss, see https://arxiv.org/abs/1512.02325.\n",
    "    '''\n",
    "\n",
    "    def __init__(self,\n",
    "                 neg_pos_ratio=3,\n",
    "                 n_neg_min=0,\n",
    "                 alpha=1.0, \n",
    "                 beta =1.0):\n",
    "        '''\n",
    "        Arguments:\n",
    "            neg_pos_ratio (int, optional): The maximum number of negative (i.e. background)\n",
    "                ground truth boxes to include in the loss computation. There are no\n",
    "                actual background ground truth boxes of course, but `y_true`\n",
    "                contains default boxes labeled with the background class. Since\n",
    "                the number of background boxes in `y_true` will ususally exceed\n",
    "                the number of positive boxes by far, it is necessary to balance\n",
    "                their influence on the loss. Defaults to 3 following the paper.\n",
    "            n_neg_min (int, optional): The minimum number of negative ground truth boxes to\n",
    "                enter the loss computation *per batch*. This argument can be used to make\n",
    "                sure that the model learns from a minimum number of negatives in batches\n",
    "                in which there are very few, or even none at all, positive ground truth\n",
    "                boxes. It defaults to 0 and if used, it should be set to a value that\n",
    "                stands in reasonable proportion to the batch size used for training.\n",
    "            alpha (float, optional): A factor to weight the localization loss in the\n",
    "                computation of the total loss. Defaults to 1.0 following the paper.\n",
    "        '''\n",
    "        self.neg_pos_ratio = tf.constant(neg_pos_ratio)\n",
    "        self.n_neg_min = tf.constant(n_neg_min)\n",
    "        self.alpha = tf.constant(alpha)\n",
    "        self.beta = tf.constant(beta)\n",
    "\n",
    "    def smooth_L1_loss(self, y_true, y_pred):\n",
    "        '''\n",
    "        Compute smooth L1 loss, see references.\n",
    "\n",
    "        Arguments:\n",
    "            y_true (nD tensor): A TensorFlow tensor of any shape containing the ground truth data.\n",
    "                In this context, the expected tensor has shape `(batch_size, #boxes, 4)` and\n",
    "                contains the ground truth bounding box coordinates, where the last dimension\n",
    "                contains `(xmin, xmax, ymin, ymax)`.\n",
    "            y_pred (nD tensor): A TensorFlow tensor of identical structure to `y_true` containing\n",
    "                the predicted data, in this context the predicted bounding box coordinates.\n",
    "\n",
    "        Returns:\n",
    "            The smooth L1 loss, a nD-1 Tensorflow tensor. In this context a 2D tensor\n",
    "            of shape (batch, n_boxes_total).\n",
    "\n",
    "        References:\n",
    "            https://arxiv.org/abs/1504.08083\n",
    "        '''\n",
    "        absolute_loss = tf.abs(y_true - y_pred)\n",
    "        square_loss = 0.5 * (y_true - y_pred)**2\n",
    "        l1_loss = tf.where(tf.less(absolute_loss, 1.0), square_loss, absolute_loss - 0.5)\n",
    "        return tf.reduce_sum(l1_loss, axis=-1)\n",
    "\n",
    "    def log_loss(self, y_true, y_pred):\n",
    "        '''\n",
    "        Compute the softmax log loss.\n",
    "\n",
    "        Arguments:\n",
    "            y_true (nD tensor): A TensorFlow tensor of any shape containing the ground truth data.\n",
    "                In this context, the expected tensor has shape (batch_size, #boxes, #classes)\n",
    "                and contains the ground truth bounding box categories.\n",
    "            y_pred (nD tensor): A TensorFlow tensor of identical structure to `y_true` containing\n",
    "                the predicted data, in this context the predicted bounding box categories.\n",
    "\n",
    "        Returns:\n",
    "            The softmax log loss, a nD-1 Tensorflow tensor. In this context a 2D tensor\n",
    "            of shape (batch, n_boxes_total).\n",
    "        '''\n",
    "        # Make sure that `y_pred` doesn't contain any zeros (which would break the log function)\n",
    "        y_pred = tf.maximum(y_pred, 1e-15)\n",
    "        # Compute the log loss\n",
    "        log_loss = -tf.reduce_sum(y_true * tf.log(y_pred), axis=-1)\n",
    "        return log_loss\n",
    "\n",
    "    def compute_loss(self, y_true, y_pred):\n",
    "        '''\n",
    "        Compute the loss of the SSD model prediction against the ground truth.\n",
    "\n",
    "        Arguments:\n",
    "            y_true (array): A Numpy array of shape `(batch_size, #boxes, #classes + 8)`,\n",
    "                where `#boxes` is the total number of boxes that the model predicts\n",
    "                per image. Be careful to make sure that the index of each given\n",
    "                box in `y_true` is the same as the index for the corresponding\n",
    "                box in `y_pred`. The last axis must have length `#classes + 8` and contain\n",
    "                `[classes one-hot encoded, 4 ground truth box coordinates, 4 arbitrary entries]`\n",
    "                in this order, including the background class. The last four entries of the\n",
    "                last axis are not used by this function and therefore their contents are\n",
    "                irrelevant, they only exist so that `y_true` has the same shape as `y_pred`,\n",
    "                where the last four entries of the last axis contain the anchor box\n",
    "                coordinates, which are needed during inference. Important: Boxes that\n",
    "                you want the cost function to ignore need to have a one-hot\n",
    "                class vector of all zeros.\n",
    "            y_pred (Keras tensor): The model prediction. The shape is identical\n",
    "                to that of `y_true`.\n",
    "\n",
    "        Returns:\n",
    "            A scalar, the total multitask loss for classification and localization.\n",
    "        '''\n",
    "        batch_size = tf.shape(y_pred)[0] # Output dtype: tf.int32\n",
    "        n_boxes = tf.shape(y_pred)[1] # Output dtype: tf.int32, note that `n_boxes` in this context denotes the total number of boxes per image, not the number of boxes per cell\n",
    "\n",
    "        # 1: Compute the losses for class and box predictions for every box\n",
    "\n",
    "        classification_loss = tf.to_float(self.log_loss(y_true[:,:,:-12], y_pred[:,:,:-12])) # Output shape: (batch_size, n_boxes)\n",
    "        localization_loss = tf.to_float(self.smooth_L1_loss(y_true[:,:,-12:-8], y_pred[:,:,-12:-8])) # Output shape: (batch_size, n_boxes)\n",
    "\n",
    "        # 2: Compute the classification losses for the positive and negative targets\n",
    "\n",
    "        # Create masks for the positive and negative ground truth classes\n",
    "        negatives = y_true[:,:,0] # Tensor of shape (batch_size, n_boxes)\n",
    "        positives = tf.to_float(tf.reduce_max(y_true[:,:,1:-12], axis=-1)) # Tensor of shape (batch_size, n_boxes)\n",
    "\n",
    "        # Count the number of positive boxes (classes 1 to n) in y_true across the whole batch\n",
    "        n_positive = tf.reduce_sum(positives)\n",
    "\n",
    "        # Now mask all negative boxes and sum up the losses for the positive boxes PER batch item\n",
    "        # (Keras loss functions must output one scalar loss value PER batch item, rather than just\n",
    "        # one scalar for the entire batch, that's why we're not summing across all axes)\n",
    "        pos_class_loss = tf.reduce_sum(classification_loss * positives, axis=-1) # Tensor of shape (batch_size,)\n",
    "\n",
    "        # Compute the classification loss for the negative default boxes (if there are any)\n",
    "\n",
    "        # First, compute the classification loss for all negative boxes\n",
    "        neg_class_loss_all = classification_loss * negatives # Tensor of shape (batch_size, n_boxes)\n",
    "        n_neg_losses = tf.count_nonzero(neg_class_loss_all, dtype=tf.int32) # The number of non-zero loss entries in `neg_class_loss_all`\n",
    "        # What's the point of `n_neg_losses`? For the next step, which will be to compute which negative boxes enter the classification\n",
    "        # loss, we don't just want to know how many negative ground truth boxes there are, but for how many of those there actually is\n",
    "        # a positive (i.e. non-zero) loss. This is necessary because `tf.nn.top-k()` in the function below will pick the top k boxes with\n",
    "        # the highest losses no matter what, even if it receives a vector where all losses are zero. In the unlikely event that all negative\n",
    "        # classification losses ARE actually zero though, this behavior might lead to `tf.nn.top-k()` returning the indices of positive\n",
    "        # boxes, leading to an incorrect negative classification loss computation, and hence an incorrect overall loss computation.\n",
    "        # We therefore need to make sure that `n_negative_keep`, which assumes the role of the `k` argument in `tf.nn.top-k()`,\n",
    "        # is at most the number of negative boxes for which there is a positive classification loss.\n",
    "\n",
    "        # Compute the number of negative examples we want to account for in the loss\n",
    "        # We'll keep at most `self.neg_pos_ratio` times the number of positives in `y_true`, but at least `self.n_neg_min` (unless `n_neg_loses` is smaller)\n",
    "        n_negative_keep = tf.minimum(tf.maximum(self.neg_pos_ratio * tf.to_int32(n_positive), self.n_neg_min), n_neg_losses)\n",
    "\n",
    "        # In the unlikely case when either (1) there are no negative ground truth boxes at all\n",
    "        # or (2) the classification loss for all negative boxes is zero, return zero as the `neg_class_loss`\n",
    "        def f1():\n",
    "            return tf.zeros([batch_size])\n",
    "        # Otherwise compute the negative loss\n",
    "        def f2():\n",
    "            # Now we'll identify the top-k (where k == `n_negative_keep`) boxes with the highest confidence loss that\n",
    "            # belong to the background class in the ground truth data. Note that this doesn't necessarily mean that the model\n",
    "            # predicted the wrong class for those boxes, it just means that the loss for those boxes is the highest.\n",
    "\n",
    "            # To do this, we reshape `neg_class_loss_all` to 1D...\n",
    "            neg_class_loss_all_1D = tf.reshape(neg_class_loss_all, [-1]) # Tensor of shape (batch_size * n_boxes,)\n",
    "            # ...and then we get the indices for the `n_negative_keep` boxes with the highest loss out of those...\n",
    "            values, indices = tf.nn.top_k(neg_class_loss_all_1D, n_negative_keep, False) # We don't need sorting\n",
    "            # ...and with these indices we'll create a mask...\n",
    "            negatives_keep = tf.scatter_nd(tf.expand_dims(indices, axis=1), updates=tf.ones_like(indices, dtype=tf.int32), shape=tf.shape(neg_class_loss_all_1D)) # Tensor of shape (batch_size * n_boxes,)\n",
    "            negatives_keep = tf.to_float(tf.reshape(negatives_keep, [batch_size, n_boxes])) # Tensor of shape (batch_size, n_boxes)\n",
    "            # ...and use it to keep only those boxes and mask all other classification losses\n",
    "            neg_class_loss = tf.reduce_sum(classification_loss * negatives_keep, axis=-1) # Tensor of shape (batch_size,)\n",
    "            return neg_class_loss\n",
    "\n",
    "        neg_class_loss = tf.cond(tf.equal(n_neg_losses, tf.constant(0)), f1, f2)\n",
    "\n",
    "        class_loss = pos_class_loss + neg_class_loss # Tensor of shape (batch_size,)\n",
    "\n",
    "        # 3: Compute the localization loss for the positive targets\n",
    "        #    We don't penalize localization loss for negative predicted boxes (obviously: there are no ground truth boxes they would correspond to)\n",
    "\n",
    "        loc_loss = tf.reduce_sum(localization_loss * positives, axis=-1) # Tensor of shape (batch_size,)\n",
    "\n",
    "        # 4: Compute the total loss\n",
    "\n",
    "        total_loss = (self.beta* class_loss + self.alpha * loc_loss) / tf.maximum(1.0, n_positive) # In case `n_positive == 0`\n",
    "\n",
    "        return total_loss\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
